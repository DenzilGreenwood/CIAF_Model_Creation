# EU Artificial Intelligence Act (Regulation (EU) 2024/1689) - Summary

**Document Version:** 1.0  
**Date:** October 18, 2025  
**Regulation Status:** In Force (August 1, 2024)  
**Official Source:** [EUR-Lex](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai)

---

## Scope & Purpose

The EU AI Act lays down harmonised rules for the development, placing on the market, putting into service, and use of AI systems in the European Union. The regulation aims to ensure human-centric, trustworthy AI while maintaining a high level of health, safety, and fundamental rights protection, while supporting innovation and the free movement of AI goods and services within the EU internal market.

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

---

## Key Definitions (High Level)

The regulation establishes critical definitions that form the foundation of its regulatory framework:

- **"AI system"** - Comprehensive definition aligning with international standards, focusing on inference capability and varying levels of autonomy
- **"Deployer"** - Entity using an AI system under its authority (except for personal non-professional use)
- **"General-purpose AI (GPAI) model"** - AI model that displays significant generality and is capable of competently performing a wide range of distinct tasks
- **"Systemic risk"** - Risk specific to high-impact capabilities of GPAI models
- **"Deep fake"** - AI-generated or manipulated image, audio, or video content

These definitions align with international work and provide legal certainty for market participants.

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

---

## Risk-Based Framework

### 1) Prohibited AI Practices (Article 5)

The following AI practices are banned across the EU, with narrow law enforcement exceptions where specifically noted:

#### Manipulation and Deception
- **Subliminal/manipulative techniques** that materially distort behavior and cause likely significant harm
- **Exploitation of vulnerabilities** due to age, disability, or social/economic situation

#### Social Scoring and Profiling
- **Social scoring** by public or private actors with cross-context or unjustified detrimental effects
- **Purely personality/profiling-based criminal risk prediction**

#### Biometric and Surveillance Restrictions
- **Untargeted scraping** of facial images to build or expand facial recognition databases
- **Emotion inference** in workplace and education contexts (unless for medical or safety purposes)
- **Biometric categorisation** to infer sensitive attributes (race, religion, sexual orientation, etc.)

#### Real-Time Biometric Identification
- **Real-time remote biometric identification** in publicly accessible spaces for law enforcement purposes

**Limited Exceptions for Law Enforcement:**
- Searching for specific victims of abduction, trafficking, or sexual exploitation
- Preventing imminent threats to life or physical safety, or genuine and present terrorist attacks
- Identifying suspects for serious crimes listed in Annex II

**Strict Safeguards Required:**
- Prior authorization from judicial or independent administrative authority
- Fundamental Rights Impact Assessment (FRIA)
- Registration in EU database

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

### 2) High-Risk AI Systems

AI systems are classified as "high-risk" through two pathways:

#### Annex I Pathway
AI systems used as safety components (or as the product itself) under existing EU product safety legislation that requires third-party conformity assessment.

#### Annex III Pathway
AI systems used in specified high-risk areas, with narrow carve-outs if the risk is not significant. The European Commission can update Annex III through delegated acts.

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

### Annex III High-Risk Use Cases (Illustrative Highlights)

#### Education
- Admissions and enrollment processes
- Student assignment to educational institutions
- Evaluation and assessment of learning outcomes
- Proctoring and examination monitoring

#### Employment
- Recruitment and selection processes
- Performance and behavior monitoring
- Task allocation and work organization
- Decisions on terms, promotion, or termination

#### Essential Services
- Eligibility determination for public benefits
- Creditworthiness assessment and credit scoring
- Life and health insurance pricing and risk assessment
- Emergency dispatch and triage systems

#### Law Enforcement
- Victim risk assessment and protection
- Polygraph and similar tools for evidence assessment
- Evidence reliability evaluation
- Risk assessment for offending or re-offending
- Criminal profiling and behavior prediction

#### Migration, Asylum & Border Control
- Security, health, or irregular migration risk assessment
- Assistance in application examination processes
- Detection and identification (except travel document verification)

#### Justice & Democratic Processes
- Tools assisting judicial decision-making
- Systems influencing voting behavior where people are directly exposed to outputs

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

---

## Obligations by Role (Selected)

### Providers of High-Risk AI (Chapter III, Section 2)

Providers must implement comprehensive compliance measures including:

#### Risk Management & Technical Requirements
- Establish and maintain risk management systems
- Implement robust data and data governance practices
- Maintain comprehensive technical documentation
- Ensure accurate record-keeping and logging

#### Transparency & Human Oversight
- Provide clear transparency and user instructions
- Design systems for effective human oversight
- Declare accuracy metrics in user instructions

#### Quality & Security
- Ensure accuracy, robustness, and cybersecurity
- Implement post-market monitoring systems
- Apply CE-marking where applicable

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

### Deployers of High-Risk AI (Article 26) & Fundamental Rights Impact Assessment (Article 27)

#### General Deployer Obligations
- Follow provider instructions for proper use
- Ensure competent human oversight is maintained
- Maintain use logs and monitoring records
- Inform affected persons of AI system use (subject to Article 50 requirements)

#### Fundamental Rights Impact Assessment (FRIA) Requirements

**Mandatory FRIA for:**
- Public bodies using high-risk AI systems
- Private entities providing public services
- Specific financial services use cases (Annex III 5(b), (c))

**FRIA Process:**
- Must be completed before first use of the system
- Notification to relevant market surveillance authority required
- FRIA complements any Data Protection Impact Assessment (DPIA) under GDPR/LED
- Does not replace other impact assessment requirements

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

### Other Market Actors

#### Importers, Distributors, Authorized Representatives
- Classic product framework responsibilities
- Verification and documentation requirements
- Cooperation with authorities obligations

#### Notified Bodies
- Independence and competence requirements
- Conformity assessment responsibilities
- Quality management system oversight

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

---

## Transparency Requirements (Article 50)

For certain non-high-risk AI systems, additional transparency duties apply:

### AI-to-Human Interaction
- **Requirement:** Inform people they are interacting with an AI system
- **Exception:** Unless it is obvious from the context

### Emotion Recognition & Biometric Categorisation
- **Requirement:** Inform natural persons when such systems are being used
- **Scope:** Applies to emotion recognition and biometric categorisation systems

### Deep Fakes & AI-Generated Content
- **Requirement:** Disclose that content is AI-generated or AI-manipulated
- **Tailored Allowances:** Special provisions for artistic, creative, satirical, or fictional works
- **Implementation:** Appropriate and clear manner suited to the content type

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

---

## General-Purpose AI (GPAI) Models (Chapter V)

### All GPAI Providers (Article 53)

#### Documentation Requirements
- Maintain comprehensive technical documentation (per Annex XI specifications)
- Provide downstream documentation (per Annex XII) enabling integrators to comply with their obligations

#### Copyright Compliance
- Implement copyright policies complying with EU copyright law
- Respect opt-out mechanisms under DSM Directive 2019/790 Article 4(3)

#### Training Data Transparency
- Publish sufficiently detailed training data summary
- Follow template provided by the AI Office
- Ensure adequate detail for downstream compliance

#### Open-Source Exception
- Certain Article 53(1)(a), (b) duties do not apply to open-source models
- Exception void if model presents systemic risk

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

### GPAI Models with Systemic Risk (Article 55)

#### Advanced Evaluation Requirements
- Conduct model evaluation including adversarial testing
- Implement continuous systemic risk assessment protocols
- Develop and maintain risk mitigation measures

#### Incident Reporting & Security
- Establish serious incident reporting procedures
- Maintain adequate cybersecurity measures
- Implement appropriate safeguards and monitoring

#### Compliance Demonstration
- Codes of practice may demonstrate compliance
- Valid until harmonised standards are developed and adopted
- Must align with regulatory expectations and best practices

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

---

## Governance & Enforcement

### EU AI Office (European Commission)
**Primary Responsibilities:**
- Lead supervision and enforcement for GPAI models
- Conduct investigations and compliance assessments
- Request documentation and system access (including via API/source code)
- Coordinate with national market surveillance authorities

### Scientific Panel
**Functions:**
- Support systemic risk determinations
- Provide qualified alerts and technical expertise
- Assist in model evaluation and risk assessment

### Market Surveillance Authorities
**National Level:**
- Oversee high-risk AI systems within national jurisdictions
- Coordinate with EU AI Office on cross-border issues

**EU Database Registration:**
- Required for certain high-risk uses
- Includes public sector deployers
- Restricted section for law enforcement and migration applications

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

---

## Penalties (Chapter XII)

Member States are required to establish penalty regimes with administrative fines up to:

### Major Violations
- **€15 million or 3% of global annual turnover** (whichever is higher)
- Applies to most operator obligations including provider and deployer duties
- Covers violations of core compliance requirements

### Information Violations
- **€7.5 million or 1% of global annual turnover** (whichever is higher)
- Applies to supplying incorrect or misleading information to authorities
- Covers documentation and reporting failures

### Special Regimes
- **EU Institutions:** Separate penalty regime administered via European Data Protection Supervisor (EDPS)
- **GPAI Cooperation:** Article 101 covers specific fines for GPAI-related cooperation failures

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

---

## Conformity Assessment, CE-Marking & Standards

### Conformity Assessment Integration
- **Sectoral Integration:** Conformity routes integrate with existing sectoral laws listed in Annex I
- **CE Marking:** Required for compliant high-risk AI systems
- **Standards Framework:** Harmonised standards and common specifications confer presumption of conformity

### Technical Standards Development
- **Harmonised Standards:** Provide technical specifications for compliance
- **Common Specifications:** Alternative technical requirements where harmonised standards are insufficient
- **Presumption of Conformity:** Systems conforming to harmonised standards presumed to meet regulatory requirements

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

---

## Entry into Force & Application Timeline (Article 113)

### Publication & Entry into Force
- **Published in Official Journal:** July 12, 2024
- **Entry into Force:** August 1, 2024 (20 days after publication)

### Staggered Application Timeline

#### February 2, 2025
- **Chapters I–II:** Definitions and prohibited practices
- **Transparency Baseline:** Basic transparency requirements

#### August 2, 2025
- **Chapter III, Section 4:** Specific high-risk system provisions
- **Chapter V:** General-Purpose AI model requirements
- **Chapter VII:** Governance structures and AI Office functions
- **Chapter XII:** Penalty frameworks
- **Article 78:** Specific enforcement provisions
- **Exception:** Article 101 (GPAI cooperation penalties) applies later

#### August 2, 2026
- **Full Application:** Complete regulatory framework in effect
- **High-Risk Systems:** All provider and deployer obligations effective
- **Conformity Assessment:** Full CE marking and notified body procedures

### Implementation Considerations
- **Grace Periods:** Some technical requirements may have additional phase-in periods
- **Market Surveillance:** National authorities must be fully operational by application dates
- **Industry Preparation:** Organizations should begin compliance preparation immediately

**Source:** [EUR-Lex](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32024R1689)

---

## Implementation Recommendations

### Immediate Actions (Now - February 2025)
1. **Compliance Mapping:** Assess current AI systems against prohibited practices
2. **Risk Classification:** Determine which systems qualify as high-risk under Annex I or III
3. **Governance Setup:** Establish AI governance structures and responsibility frameworks

### Short-Term Preparation (February 2025 - August 2025)
1. **GPAI Compliance:** Implement documentation and transparency requirements for general-purpose AI models
2. **Policy Development:** Develop internal policies for prohibited practices compliance
3. **Training Programs:** Educate staff on transparency and disclosure requirements

### Medium-Term Implementation (August 2025 - August 2026)
1. **High-Risk System Preparation:** Develop comprehensive compliance frameworks for high-risk AI systems
2. **Quality Management:** Implement required QMS for high-risk AI providers
3. **Documentation Systems:** Establish technical documentation and record-keeping systems
4. **FRIA Procedures:** Develop Fundamental Rights Impact Assessment capabilities for relevant deployers

### Long-Term Compliance (August 2026 onwards)
1. **Full Operational Compliance:** All systems and processes fully aligned with regulatory requirements
2. **Continuous Monitoring:** Ongoing post-market surveillance and compliance monitoring
3. **Conformity Assessment:** Complete CE marking and notified body assessment processes
4. **Incident Response:** Fully operational incident reporting and response procedures

---

**Document Control:**
- **Document Owner:** CIAF Regulatory Compliance Team
- **Next Review Date:** February 18, 2026
- **Related Documents:** EU AI Act Full Text, Implementation Guidelines, Technical Standards
- **Version History:** v1.0 - Initial summary (October 18, 2025)