\documentclass[12pt,a4paper]{article}

% CIAF unified styles package - provides all formatting, colors, and box types
\usepackage{../styles/ciaf_document_styles}

% Document setup using CIAF styles package
\setupdocument{CIAF Data Structures: Technical Specification for Lazy Capsule Materialization}{Technical Specification}{Denzil James Greenwood}{v1.0.0}
\setupheaderfooter{CIAF Data Structures}{Technical Specification}

% Document content starts below

% Code listing setup
\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{ciafblue}\bfseries,
    commentstyle=\color{ciafgray}\itshape,
    stringstyle=\color{ciaforange},
    numberstyle=\tiny\color{ciafgray},
    backgroundcolor=\color{gray!5},
    frame=single,
    frameround=tttt,
    framerule=0.5pt,
    rulecolor=\color{ciafgray!50},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4,
    showstringspaces=false,
    numbers=left,
    numbersep=8pt,
    xleftmargin=15pt,
    xrightmargin=5pt,
    captionpos=b,
    keepspaces=true,
    showspaces=false,
    showtabs=false,
    stepnumber=1,
    stringstyle=\color{red},
    tabsize=2,
    title=\lstname
}



\begin{document}

% Title page
\begin{titlepage}
\centering
\vspace*{2cm}

{\Huge\bfseries CIAF Data Structures: Technical Specification for Lazy Capsule Materialization\par}
\vspace{1cm}
{\Large A Comprehensive Analysis of Core Data Structures in the Cognitive Insight AI Framework\par}
\vspace{2cm}

{\Large\textbf{Author:} Denzil James Greenwood\par}
\vspace{0.5cm}
{\large\textbf{Institution:} Independent Researcher\par}
\vspace{0.5cm}
{\large\textbf{Date:} October 22, 2025\par}
\vspace{0.5cm}
{\large\textbf{Version:} 1.0\par}

\vfill

\begin{center}
\fbox{\begin{minipage}{0.8\textwidth}
\textbf{Technical Notice:} This document provides authoritative technical specifications for all data structures used in the Cognitive Insight AI Framework (CIAF) and Lazy Capsule Materialization (LCM\texttrademark) process. All data structures, schemas, and implementation patterns are derived from production codebase analysis and represent the canonical reference for CIAF implementation.
\end{minipage}}
\end{center}

\vfill
\end{titlepage}

% Abstract
\begin{abstract}
The Cognitive Insight AI Framework (CIAF) implements a sophisticated data structure hierarchy to support Lazy Capsule Materialization (LCM\texttrademark) and comprehensive AI governance. This technical specification provides complete documentation of all core data structures, their relationships, validation rules, and implementation patterns based on comprehensive codebase analysis.

The framework's data architecture supports eight canonical stages of AI lifecycle management: dataset anchoring, model anchoring, training sessions, pre-deployment validation, production deployment, test evaluation, inference processing, and Merkle root management. Each stage utilizes specialized data structures optimized for cryptographic verification, storage efficiency, and regulatory compliance.

This document serves as the authoritative reference for implementers, auditors, and researchers working with CIAF data structures, providing complete specifications for lightweight receipts, capsule headers, commitment schemes, policy frameworks, and verification protocols.

\textbf{Keywords:} Data Structures, AI Governance, Lazy Materialization, Cryptographic Verification, Schema Design, Framework Architecture
\end{abstract}

\begin{technicalbox}
\textbf{CIAF Canonical Naming Standards (from Variables Reference)}
\begin{itemize}
\item \textbf{Variables/functions/modules:} snake\_case
\item \textbf{Classes/enums:} PascalCase  
\item \textbf{Enum members:} UPPER\_CASE; serialized values: lower-case tokens
\item \textbf{Anchors:} *\_anchor (object/bytes), *\_anchor\_hex (hex), *\_anchor\_ref (opaque ID)
\item \textbf{Times:} receipts → committed\_at (RFC 3339 Z); capsules → generated\_at
\item \textbf{Merkle path:} List[[hash:str, position:"left"|"right"]]
\item \textbf{Correlation:} request\_id (accept operation\_id as alias; normalize on ingest)
\end{itemize}
\end{technicalbox}

\begin{infobox}
\textbf{Canonical JSON for Hashing (Normative)}
\begin{itemize}
\item Serialize with sorted keys, no spaces, ASCII:
\item \texttt{json.dumps(obj, sort\_keys=True, separators=(",", ":"), ensure\_ascii=True, default=str)}
\item Hash result with SHA-256 (requirement, not example)
\end{itemize}
\end{infobox}

\newpage
\tableofcontents
\newpage

\section{Introduction}

\subsection{Document Purpose and Scope}

This technical specification provides comprehensive documentation of all data structures used in the Cognitive Insight AI Framework (CIAF) and its core Lazy Capsule Materialization (LCM\texttrademark) process. The specifications are derived from production codebase analysis and represent the canonical reference for CIAF implementation across diverse computing environments and regulatory contexts.

The data structure architecture supports the complete AI governance lifecycle through eight canonical stages, each with specialized data types optimized for specific operational requirements while maintaining cryptographic integrity and regulatory compliance.

\subsection{Data Structure Hierarchy Overview}

The CIAF data structure hierarchy implements a layered architecture with clear separation of concerns:

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=0.3cm and 0.4cm,
    rect/.style={rectangle, draw=black, thick, rounded corners=2pt, minimum width=1.4cm, minimum height=0.6cm, text centered, font=\scriptsize, align=center},
    policy/.style={rect, fill=blue!15, draw=blue!60},
    anchors/.style={rect, fill=green!15, draw=green!60},
    receipt/.style={rect, fill=orange!15, draw=orange!60},
    capsule/.style={rect, fill=purple!15, draw=purple!60},
    arrow/.style={->, thick, color=gray!70}
]

% Horizontal inline layout - all nodes in a single row with compact spacing
\node[policy] (policy) {Policy\\Framework};
\node[anchors, right=of policy] (dataset) {Dataset\\Anchors};
\node[receipt, right=of dataset] (lightweight) {Lightweight\\Receipts};
\node[anchors, right=of lightweight] (model) {Model\\Anchors};
\node[receipt, right=of model] (training) {Training\\Sessions};
\node[anchors, right=of training] (deployment) {Deployment\\Anchors};
\node[receipt, right=of deployment] (inference) {Inference\\Receipts};
\node[capsule, right=of inference] (capsule) {Capsule\\Headers};

% Policy governance arch - showing policy defines and checks compliance for anchors
% Higher arches that go over intermediate boxes
\draw[draw=blue!60, thick, dashed, bend left=60] (policy.north) to node[above=0.825cm, font=\small, text=blue!70] {Policy Governance \& Compliance Checks} (dataset.north);
\draw[draw=blue!60, thick, dashed, bend left=45] (policy.north) to (model.north);
\draw[draw=blue!60, thick, dashed, bend left=30] (policy.north) to (deployment.north);

% Arrows showing ML lifecycle flow
\draw[arrow] (policy) -- (dataset);
\draw[arrow] (dataset) -- (lightweight);
\draw[arrow] (lightweight) -- (model);
\draw[arrow] (model) -- (training);
\draw[arrow] (training) -- (deployment);
\draw[arrow] (deployment) -- (inference);
\draw[arrow] (inference) -- (capsule);

\end{tikzpicture}
\caption{CIAF Data Structure Hierarchy}
\label{fig:hierarchy}
\end{figure}

\subsubsection{Policy Framework}

The Policy Framework constitutes the foundational layer that establishes cryptographic governance and behavioral constraints across all CIAF operations. The governance arch in Figure 1 illustrates how policy defines and enforces compliance across the three critical anchor types:

\begin{itemize}
    \item \textbf{Cryptographic Policies:} Defines hash algorithms (SHA-256, SHA-3), signature schemes (ECDSA, EdDSA), and canonicalization rules (JSON sorted UTF-8) that ensure consistent cryptographic operations across distributed environments.
    
    \item \textbf{Domain Specifications:} Establishes domain boundaries for DATASET, MODEL, DEPLOYMENT, and INFERENCE operations with distinct commitment types and verification requirements.
    
    \item \textbf{Protocol Implementations:} Configures swappable cryptographic backends through standardized interfaces, enabling adaptation to hardware security modules, cloud KMS, or specialized cryptographic accelerators.
    
    \item \textbf{Compliance Parameters:} Sets regulatory alignment parameters for GDPR, HIPAA, SOX, and custom frameworks through flexible metadata schemas and evidence retention policies.
    
    \item \textbf{Policy Governance Arch:} The governance arch demonstrates how policy framework defines and enforces compliance checks at Dataset Anchors, Model Anchors, and Deployment Anchors. Each anchor creation triggers policy validation to ensure regulatory compliance, cryptographic integrity, and organizational constraints are met before anchoring.
\end{itemize}

\subsubsection{Anchor Structures}

Core anchoring data structures provide cryptographic binding and immutable lifecycle tracking for critical AI assets, with policy compliance enforced at creation time:

\begin{itemize}
    \item \textbf{Dataset Anchors:} Capture dataset fingerprints through Merkle tree construction over data samples, including provenance chains, quality metrics, and transformation histories. Support both structured (tabular) and unstructured (images, text) data with content-aware hashing. Policy compliance validation ensures data privacy requirements, retention policies, and regulatory constraints are enforced during anchor creation.
    
    \item \textbf{Model Anchors:} Secure model artifacts through weight serialization hashing, architecture fingerprinting, and hyperparameter binding. Include model lineage tracking, version management, and compatibility matrices for distributed deployment scenarios. Policy enforcement validates model authorization against approved datasets and ensures regulatory compliance for AI model deployment.
    
    \item \textbf{Deployment Anchors:} Bind model deployments to specific environments through infrastructure fingerprinting, configuration hashing, and runtime constraint verification. Track deployment topology, scaling parameters, and performance benchmarks. Policy checks validate deployment environment compliance, security configurations, and operational constraints before production deployment.
\end{itemize}

\subsubsection{Receipt Structures}

Operational data structures optimized for high-frequency evidence capture during AI system operations:

\begin{itemize}
    \item \textbf{Lightweight Receipts:} Minimal cryptographic receipts for high-throughput operations, using optimized binary serialization and compressed evidence formats. Support batch verification and deferred materialization for storage efficiency.
    
    \item \textbf{Inference Receipts:} Comprehensive evidence capture for inference operations including input hashes, model state verification, output commitments, and performance metrics. Include bias detection markers and uncertainty quantification data.
    
    \item \textbf{Training Sessions:} Complete training lifecycle documentation with epoch-level checkpoints, gradient summaries, loss function evolution, and convergence metrics. Support distributed training coordination and reproducibility verification.
\end{itemize}

\subsubsection{Integration Structures}

Comprehensive data structures that aggregate multiple evidence sources into cohesive audit packages:

\begin{itemize}
    \item \textbf{Capsule Headers:} Unified containers that combine anchors, receipts, and metadata into complete audit packages. Include cross-referencing capabilities, compliance report generation, and regulatory submission formatting.
    
    \item \textbf{Evidence Chains:} Temporal sequences of receipts and anchors that demonstrate complete AI system lifecycle compliance from data ingestion through inference deployment.
    
    \item \textbf{Audit Packages:} Regulatory-ready evidence bundles with standardized schemas, digital signatures, and verification instructions for external audit processes.
\end{itemize}

\subsection{Design Principles}

The CIAF data structure design implements several key principles:

\textbf{Cryptographic Integrity:} All data structures include cryptographic hashes, digital signatures, and Merkle proofs to ensure tamper-evidence and verification capability.

\textbf{Storage Efficiency:} Lightweight receipt patterns and deferred materialization minimize storage overhead while preserving complete audit capabilities.

\textbf{Regulatory Compliance:} Schema design accommodates diverse regulatory frameworks through flexible metadata structures and standardized evidence formats.

\textbf{Implementation Flexibility:} Protocol-based interfaces enable swappable cryptographic implementations while maintaining structural consistency.

\section{Core Policy Framework}

\subsection{LCM Policy Structure}

The LCM Policy serves as the foundational configuration structure that governs all cryptographic operations and data structure behavior throughout the framework.

\begin{lstlisting}[language=Python, caption=LCM Policy Data Structure]
@dataclass  
class LCMPolicy:
    """
    Comprehensive CIAF LCM policy defining all cryptographic and structural policies.
    """
    
    # Policy identification
    policy_id: str = "ciaf_default_lcm_policy"
    
    # Core cryptographic policy
    hash_algorithm: str = "sha256"
    canonicalization: str = "json_sorted_utf8"
    domains: List[DomainType] = None
    merkle: MerklePolicy = None
    commitments: CommitmentType = CommitmentType.SALTED
    
    # Schema versions for compatibility
    anchor_schema_version: str = "1.0"
    merkle_policy_version: str = "1.0"
    
    # Protocol implementations (dependency injection)
    rng: Optional[RNG] = None
    anchor_deriver: Optional[AnchorDeriver] = None
    anchor_store: Optional[AnchorStore] = None
    signer: Optional[Signer] = None
    merkle_factory: Optional[Any] = None
\end{lstlisting}

\subsubsection{Domain Type Enumeration}

The domain type system provides structured categorization for all anchored artifacts:

\begin{lstlisting}[language=Python, caption=Domain Type Enumeration]
class DomainType(Enum):
    """CIAF domain types for anchoring classification."""
    DATASET = "CIAF|dataset"
    DATASET_FAMILY = "CIAF|dataset|family"
    DATASET_SPLIT = "CIAF|dataset|split"
    MODEL = "CIAF|model" 
    TRAIN = "CIAF|train"
    DEPLOYMENT = "CIAF|deployment"
    INFERENCE = "CIAF|inference"
\end{lstlisting}

\subsubsection{Commitment Type Framework}

The commitment type system enables privacy-preserving audit trails through configurable commitment schemes:

\begin{lstlisting}[language=Python, caption=Commitment Type Framework]
class CommitmentType(Enum):
    """Commitment algorithms for privacy protection."""
    SALTED = "salted"           # SHA-256 with cryptographic salt
    HMAC_SHA256 = "hmac_sha256" # HMAC-based commitment
    PLAINTEXT = "plaintext"     # For non-sensitive data
\end{lstlisting}

\subsubsection{Merkle Policy Configuration}

Merkle tree construction parameters ensure consistent cryptographic verification across all operations:

\begin{lstlisting}[language=Python, caption=Merkle Policy Configuration]
@dataclass
class MerklePolicy:
    """Merkle tree construction policy."""
    fanout: int = 2                    # Binary tree structure
    padding: str = "duplicate_last"    # Padding strategy for incomplete levels
    leaf_encoding: str = "raw32"       # 32-byte raw hash encoding
\end{lstlisting}

\section{Lightweight Receipt Structures}

\subsection{Deferred LCM Receipt}

The lightweight receipt represents the core innovation of the LCM process, providing minimal storage overhead while preserving complete audit reconstruction capability:

\begin{lstlisting}[language=Python, caption=Lightweight Receipt Structure]
@dataclass
class LightweightReceipt:
    """Minimal receipt stored during fast inference operations."""
    
    # Core identification
    receipt_id: str                        # UUID v4 identifier
    committed_at: str                      # RFC 3339 timestamp with Z
    request_id: str                        # Request correlation ID
    
    # Model and version tracking
    model_anchor_ref: str                  # Model anchor reference
    model_version: str                     # Specific model version
    
    # Cryptographic commitments
    input_hash: str                        # SHA-256 of input data
    output_hash: str                       # SHA-256 of output data
    input_commitment: str                  # Privacy-preserving input commitment
    output_commitment: str                 # Privacy-preserving output commitment
    
    # Optional encrypted data (for audit materialization)
    raw_input: Optional[str] = None        # Encrypted input data
    raw_output: Optional[str] = None       # Encrypted output data
    
    # Operational metadata
    priority: str = "normal"               # Processing priority
    metadata: Optional[Dict] = None        # Extensible metadata
\end{lstlisting}

\subsubsection{Storage Efficiency Analysis}

The lightweight receipt achieves significant storage reduction compared to traditional audit approaches:

\begin{align}
\text{Traditional Receipt Size} &= \text{Input Data} + \text{Output Data} + \text{Metadata} \\
&\approx 50\text{KB to 5MB per operation} \\
\\
\text{Lightweight Receipt Size} &= \text{Hashes} + \text{Commitments} + \text{References} \\
&\approx 500\text{ bytes to 1KB per operation} \\
\\
\text{Storage Reduction Ratio} &= \frac{\text{Traditional Size}}{\text{Lightweight Size}} \\
&\approx 100:1 \text{ to } 5000:1
\end{align}

\subsection{Enhanced Receipt Schema}

The enhanced receipt provides comprehensive validation and regulatory compliance features through strict schema enforcement:

\begin{lstlisting}[language=Python, caption=Enhanced Receipt Base Structure]
class BaseReceipt(BaseModel):
    """Base receipt with comprehensive validation."""
    
    # Core identification with validation
    receipt_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    evidence_strength: EvidenceStrength = EvidenceStrength.REAL
    committed_at: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'))
    
    @field_validator('receipt_id')
    @classmethod
    def validate_receipt_id(cls, v):
        """Validate UUID format for receipt ID."""
        try:
            uuid.UUID(v)
            return v
        except ValueError:
            raise ValueError('receipt_id must be a valid UUID')
\end{lstlisting}

\subsubsection{Evidence Strength Classification}

The evidence strength system provides transparency about data provenance and reliability:

\begin{lstlisting}[language=Python, caption=Evidence Strength Enumeration]
class EvidenceStrength(str, Enum):
    """Evidence reliability classification."""
    REAL = "real"           # Production data from actual operations
    SIMULATED = "simulated" # Realistic simulated data for testing
    FALLBACK = "fallback"   # Fallback data when primary collection fails
\end{lstlisting}

\section{Anchor Data Structures}

\subsection{Dataset Anchor Framework}

Dataset anchors provide comprehensive tracking for data provenance and lineage throughout the AI lifecycle:

\begin{lstlisting}[language=Python, caption=Dataset Anchor Structure]
@dataclass
class LCMDatasetAnchor:
    """Comprehensive dataset anchor with cryptographic binding."""
    
    # Core identification
    anchor_id: str                         # Derived anchor identifier
    dataset_id: str                        # Original dataset identifier
    dataset_hash: str                      # SHA-256 of dataset content
    
    # Metadata and provenance
    metadata: DatasetMetadata              # Structured metadata
    source_info: Dict[str, Any]            # Data source information
    
    # Cryptographic verification
    merkle_root: str                       # Merkle root for batch verification
    signature: str                         # Digital signature for authenticity
    
    # Policy compliance
    policy: LCMPolicy                      # Governing policy
    timestamp: str                         # Creation timestamp
\end{lstlisting}

\subsubsection{Dataset Metadata Structure}

Dataset metadata provides comprehensive information for regulatory compliance and data governance:

\begin{lstlisting}[language=Python, caption=Dataset Metadata Structure]
@dataclass
class DatasetMetadata:
    """Comprehensive dataset metadata for governance."""
    
    # Basic properties
    shape: Tuple[int, int]                 # Dataset dimensions (rows, columns)
    dtypes: Dict[str, str]                 # Column data types
    size_bytes: int                        # Storage size in bytes
    
    # Data quality metrics
    null_counts: Dict[str, int]            # Null value counts per column
    unique_counts: Dict[str, int]          # Unique value counts per column
    
    # Provenance tracking
    source_hash: str                       # Hash of source data
    processing_steps: List[str]            # Applied transformations
    validation_results: Dict[str, Any]     # Data validation outcomes
    
    # Compliance metadata
    privacy_classification: str           # Data privacy level
    retention_policy: str                 # Data retention requirements
    access_controls: List[str]            # Access control specifications
\end{lstlisting}

\subsection{Model Anchor Framework}

Model anchors capture complete model state and configuration for reproducible AI operations:

\begin{lstlisting}[language=Python, caption=Model Anchor Structure]
@dataclass
class LCMModelAnchor:
    """Comprehensive model anchor with state binding."""
    
    # Core identification
    anchor_id: str                         # Derived anchor identifier
    model_id: str                          # Model identifier
    model_hash: str                        # Hash of model parameters
    
    # Model configuration
    architecture: Dict[str, Any]           # Model architecture specification
    hyperparameters: Dict[str, Any]        # Training hyperparameters
    framework_info: Dict[str, str]         # ML framework versions
    
    # Training provenance
    dataset_anchor_ref: str                # Reference to training dataset anchor
    training_session_ref: str              # Reference to training session
    
    # Performance metrics
    validation_metrics: Dict[str, float]   # Model validation results
    benchmark_results: Dict[str, Any]      # Benchmark performance data
    
    # Cryptographic verification
    merkle_root: str                       # Merkle root for verification
    signature: str                         # Digital signature
    policy: LCMPolicy                      # Governing policy
\end{lstlisting}

\subsection{Deployment Anchor Framework}

Deployment anchors track model deployment configurations and operational parameters:

\begin{lstlisting}[language=Python, caption=Deployment Anchor Structure]
@dataclass
class LCMDeploymentAnchor:
    """Production deployment anchor with operational binding."""
    
    # Core identification
    anchor_id: str                         # Deployment anchor identifier
    deployment_id: str                     # Deployment instance identifier
    
    # Model binding
    model_anchor_ref: str                  # Reference to deployed model
    predeployment_anchor_ref: str          # Reference to pre-deployment validation
    
    # Deployment configuration
    environment_config: Dict[str, Any]     # Deployment environment settings
    scaling_config: Dict[str, Any]         # Auto-scaling configuration
    monitoring_config: Dict[str, Any]      # Monitoring and alerting setup
    
    # Operational parameters
    performance_targets: Dict[str, float]  # SLA and performance targets
    resource_limits: Dict[str, Any]        # Resource allocation limits
    
    # Security configuration
    access_controls: List[str]             # Access control policies
    encryption_config: Dict[str, str]      # Encryption configuration
    
    # Verification and compliance
    deployment_timestamp: str              # Deployment timestamp
    merkle_root: str                       # Verification Merkle root
    signature: str                         # Deployment signature
    policy: LCMPolicy                      # Governing policy
\end{lstlisting}

\section{Inference and Training Structures}

\subsection{LCM Inference Receipt}

The LCM inference receipt provides comprehensive audit capabilities for individual inference operations:

\begin{lstlisting}[language=Python, caption=LCM Inference Receipt Structure]
class LCMInferenceReceipt:
    """Enhanced inference receipt for comprehensive audit trails."""
    
    def __init__(
        self,
        receipt_id: str,                   # Unique receipt identifier
        model_anchor_ref: str,             # Model anchor reference
        deployment_anchor_ref: str,        # Deployment anchor reference
        request_id: str,                   # Request correlation ID
        query: str,                        # Input query/prompt
        ai_output: str,                    # AI model output
        input_commitment: LCMInferenceCommitment,   # Input commitment
        output_commitment: LCMInferenceCommitment,  # Output commitment
        explanation_digests: List[str] = None,      # Explanation hashes
        prev_connections_digest: str = None,        # Connections digest
        policy: LCMPolicy = None           # Governing policy
    ):
        # Initialize core properties
        self.receipt_id = receipt_id
        self.model_anchor_ref = model_anchor_ref
        self.deployment_anchor_ref = deployment_anchor_ref
        self.request_id = request_id
        self.query = query
        self.ai_output = ai_output
        self.input_commitment = input_commitment
        self.output_commitment = output_commitment
        self.explanation_digests = explanation_digests or []
        self.prev_connections_digest = prev_connections_digest
        self.policy = policy or get_default_policy()
        self.timestamp = datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')
        
        # Computed cryptographic properties
        self.receipt_digest = self._compute_receipt_digest()
        self.connections_digest = self._compute_connections_digest()
        self.anchor_id = f"r_{self.receipt_digest[:8]}..."
\end{lstlisting}

\subsubsection{Inference Commitment Structure}

Inference commitments provide privacy protection for sensitive input and output data:

\begin{lstlisting}[language=Python, caption=Inference Commitment Structure]
@dataclass
class LCMInferenceCommitment:
    """Privacy-preserving commitment for inference data."""
    
    commitment_type: CommitmentType        # Commitment algorithm type
    commitment_value: str                  # Computed commitment value
    metadata: Dict[str, Any] = None        # Additional commitment metadata
    
    def __post_init__(self):
        """Initialize metadata if not provided."""
        if self.metadata is None:
            self.metadata = {
                "created_at": datetime.now(timezone.utc).isoformat(),
                "algorithm_params": {},
                "verification_hints": {}
            }
\end{lstlisting}

\subsection{Training Session Structure}

Training sessions capture comprehensive information about model training processes:

\begin{lstlisting}[language=Python, caption=Training Session Structure]
@dataclass
class LCMTrainingSession:
    """Comprehensive training session with audit trail."""
    
    # Core identification
    session_id: str                        # Training session identifier
    anchor_id: str                         # Derived anchor identifier
    
    # Training configuration
    model_anchor_ref: str                  # Reference to model anchor
    dataset_anchor_ref: str                # Reference to training dataset
    hyperparameters: Dict[str, Any]        # Training hyperparameters
    
    # Training progress tracking
    epochs_completed: int                  # Number of completed epochs
    training_metrics: Dict[str, List[float]]  # Training metrics by epoch
    validation_metrics: Dict[str, List[float]]  # Validation metrics by epoch
    
    # Checkpoint management
    checkpoint_hashes: List[str]           # Hashes of model checkpoints
    best_checkpoint_hash: str              # Hash of best performing checkpoint
    
    # Environment and reproducibility
    random_seeds: Dict[str, int]           # Random seeds for reproducibility
    environment_info: Dict[str, str]       # Environment configuration
    framework_versions: Dict[str, str]     # ML framework versions
    
    # Audit and verification
    training_start: str                    # Training start timestamp
    training_end: str                      # Training completion timestamp
    merkle_root: str                       # Training verification root
    signature: str                         # Digital signature
    policy: LCMPolicy                      # Governing policy
\end{lstlisting}

\section{Capsule Header Integration}

\subsection{Comprehensive Capsule Structure}

The capsule header provides complete integration of all CIAF components into a single, verifiable audit package:

\begin{lstlisting}[language=Python, caption=Capsule Header Structure]
@dataclass
class CapsuleHeader:
    """
    Comprehensive CIAF LCM state capsule for complete audit trails.
    """
    
    # Core metadata
    capsule_version: str                   # Capsule format version
    generated_at: str                      # Generation timestamp
    policy: Dict[str, Any]                 # Serialized policy configuration
    
    # Canonical stage anchors (A-H stages)
    stage_a_dataset: Optional[Dict[str, Any]] = None       # Dataset anchor
    stage_b_model: Optional[Dict[str, Any]] = None         # Model anchor
    stage_c_training: Optional[Dict[str, Any]] = None      # Training session
    stage_d_predeployment: Optional[Dict[str, Any]] = None # Pre-deployment
    stage_e_deployment: Optional[Dict[str, Any]] = None    # Deployment
    stage_f_test_evaluation: Optional[Dict[str, Any]] = None # Test evaluation
    stage_g_inference: Optional[Dict[str, Any]] = None     # Inference receipt
    stage_h_roots: Optional[Dict[str, Any]] = None         # Merkle roots
    
    def to_json(self, indent: int = 2) -> str:
        """Convert to pretty-printed JSON for human readability."""
        return json.dumps(asdict(self), indent=indent, sort_keys=True)
    
    def to_compact_json(self) -> str:
        """Convert to compact JSON for storage efficiency."""
        return json.dumps(asdict(self), separators=(',', ':'), sort_keys=True)
\end{lstlisting}

\subsubsection{Stage-Based Architecture}

The capsule header implements a canonical eight-stage architecture that corresponds to the complete AI lifecycle:

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.5cm and 0.8cm,
    stage/.style={rectangle, draw=blue!60, thick, rounded corners=5pt, minimum width=2.2cm, minimum height=1cm, text centered, font=\scriptsize, fill=blue!10, align=center},
    arrow/.style={->, thick, color=blue!70}
]

% Top row
\node[stage] (a) {Stage A\\ Dataset};
\node[stage, right=of a] (b) {Stage B\\ Model};
\node[stage, right=of b] (c) {Stage C\\ Training};
\node[stage, right=of c] (d) {Stage D\\ Pre-Deploy};

% Bottom row
\node[stage, below=of d] (e) {Stage E\\ Deployment};
\node[stage, left=of e] (f) {Stage F\\ Test Eval};
\node[stage, left=of f] (g) {Stage G\\ Inference};
\node[stage, left=of g] (h) {Stage H\\ Roots};

% Arrows showing flow
\draw[arrow] (a) -- (b);
\draw[arrow] (b) -- (c);
\draw[arrow] (c) -- (d);
\draw[arrow] (d) -- (e);
\draw[arrow] (e) -- (f);
\draw[arrow] (f) -- (g);
\draw[arrow] (g) -- (h);

\end{tikzpicture}
\caption{Eight-Stage CIAF Lifecycle Architecture}
\label{fig:stages}
\end{figure}

\section{Protocol Interface Specifications}

\subsection{Cryptographic Protocol Interfaces}

The framework defines protocol interfaces for swappable cryptographic implementations:

\begin{lstlisting}[language=Python, caption=Signer Protocol Interface]
@runtime_checkable
class Signer(Protocol):
    """Protocol for digital signature implementations."""
    
    key_id: str                            # Signer key identifier
    
    def sign(self, data: bytes) -> str:
        """Sign data and return signature string."""
        ...
    
    def verify(self, data: bytes, signature: str) -> bool:
        """Verify signature against data."""
        ...
\end{lstlisting}

\begin{center}
\fbox{\begin{minipage}{0.9\textwidth}
\textbf{Signature Payload Rule (Normative)}
\begin{itemize}
\item \textbf{Receipt signing:} sig = Ed25519( canonical\_receipt || committed\_at )
\item \textbf{Batch root signing:} sig\_root = Ed25519( merkle\_root || rfc3161\_ts\_token )
\item \textbf{Verification:} MUST validate the Ed25519 signature and, when present, the RFC 3161 token
\end{itemize}
\end{minipage}}
\end{center}

\begin{lstlisting}[language=Python, caption=Merkle Protocol Interface]
@runtime_checkable
class Merkle(Protocol):
    """Protocol for Merkle tree implementations."""
    
    def add_leaf(self, leaf_hash: str) -> str:
        """Add leaf and return new root hash."""
        ...
    
    def get_root(self) -> str:
        """Get the current Merkle root hash."""
        ...
    
    def get_proof(self, leaf_hash: str) -> List[Tuple[str, str]]:
        """Get inclusion proof as (hash, position) tuples."""
        ...
    
    def verify_proof(self, leaf_hash: str, proof: List[Tuple[str, str]], root: str) -> bool:
        """Verify Merkle inclusion proof."""
        ...
\end{lstlisting}

\subsection{Storage Protocol Interfaces}

Storage protocols enable flexible backend implementations while maintaining consistent data structure behavior:

\begin{lstlisting}[language=Python, caption=Anchor Store Protocol Interface]
@runtime_checkable
class AnchorStore(Protocol):
    """Protocol for anchor storage implementations."""
    
    def append_anchor(self, anchor: Dict[str, Any]) -> None:
        """Append new anchor with WORM semantics."""
        ...
    
    def get_latest_anchor(self) -> Optional[Dict[str, Any]]:
        """Get the most recent anchor from store."""
        ...
\end{lstlisting}

\section{JSON Schema Specifications}

\subsection{Capsule Schema}

\textbf{Capsule Representation Consistency:} The CapsuleHeader (stages A–H) is the canonical internal representation for capsule construction and processing. For external proof exchange and validation, capsules MUST be serialized as audit\_proof objects that contain \{record, proofs, anchor, verification\} per the capsule.schema.json specification below.

The JSON schema provides formal validation for capsule data structures:

\begin{lstlisting}[caption=Capsule JSON Schema]
{
  "$id": "https://cognitiveinsight.ai/schemas/capsule.schema.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "CIAF Audit Capsule",
  "type": "object",
  "required": [
    "capsule_version",
    "capsule_type", 
    "timestamp",
    "record",
    "proofs",
    "anchor",
    "verification"
  ],
  "properties": {
    "capsule_version": { "type": "string", "const": "1.0" },
    "capsule_type": { "type": "string", "const": "audit_proof" },
    "timestamp": { "type": "string", "format": "date-time" },
    "record": {
      "type": "object",
      "required": ["type", "metadata", "leaf_hash"],
      "properties": {
        "type": { 
          "enum": [
            "dataset", "model", "inference", 
            "anchor", "monitoring", "compliance"
          ] 
        },
        "metadata": { "type": "object" },
        "leaf_hash": { "type": "string" }
      }
    },
    "proofs": {
      "type": "object",
      "required": ["merkle_path", "merkle_root", "inclusion_proof_valid"],
      "properties": {
        "merkle_path": { 
          "type": "array", 
          "items": { 
            "type": "array", 
            "prefixItems": [
              { "type": "string" }, 
              { "enum": ["left", "right"] }
            ], 
            "minItems": 2, 
            "maxItems": 2 
          } 
        },
        "merkle_root": { "type": "string" },
        "inclusion_proof_valid": { "type": "boolean" }
      }
    }
  }
}
\end{lstlisting}

\subsection{Object \& Field Map}

Complete mapping between CapsuleHeader (internal) and audit\_proof (external) representations:

\begin{table}[H]
\centering
\caption{CapsuleHeader to audit\_proof Field Mapping}
\begin{tabular}{|l|l|l|}
\hline
\textbf{CapsuleHeader Field} & \textbf{audit\_proof Field} & \textbf{Notes} \\
\hline
stage\_a\_dataset & record.dataset\_anchor\_ref & Dataset identification \\
stage\_b\_model & record.model\_anchor\_ref & Model identification \\
stage\_c\_training & record.training\_metadata & Training session data \\
stage\_d\_validation & record.validation\_results & Pre-deployment validation \\
stage\_e\_deployment & record.deployment\_anchor\_ref & Deployment reference \\
stage\_f\_testing & record.test\_metadata & Test evaluation data \\
stage\_g\_inference & record.inference\_data & Inference operation data \\
stage\_h\_roots & proofs.merkle\_path & Merkle tree proofs \\
capsule\_signature & verification.signature & Digital signature \\
generated\_at & verification.timestamp & Creation timestamp \\
capsule\_id & anchor.capsule\_id & Unique identifier \\
\hline
\end{tabular}
\end{table}

\section{Data Flow and Relationships}

\subsection{Lifecycle Data Flow}

The complete data flow through the CIAF system demonstrates the relationships between data structures. The diagram below shows three distinct phases of the CIAF lifecycle:

\begin{itemize}[itemsep=0pt]
\item \textbf{Training Phase}: Dataset anchors define model anchors, which are used in training sessions
\item \textbf{Deployment Phase}: Training sessions create deployment anchors that generate inference receipts
\item \textbf{Storage \& Access Phase}: Inference receipts produce lightweight receipts that are stored in capsule headers, with materialization and integration processes providing data access
\end{itemize}

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=2cm and 2.5cm,
    data/.style={rectangle, draw=ciafblue, thick, rounded corners=3pt, minimum width=2.2cm, minimum height=0.8cm, text centered, font=\scriptsize, fill=ciaflight, align=center},
    process/.style={diamond, draw=ciaforange, thick, minimum width=1.8cm, minimum height=1cm, text centered, font=\scriptsize, fill=ciaforange!10, align=center, aspect=2},
    arrow/.style={->, thick, color=ciafblue}
]

% Top row - Data Creation Phase
\node[data] (dataset) {Dataset\\ Anchor};
\node[data, right=of dataset] (model) {Model\\ Anchor};
\node[data, right=of model] (training) {Training\\ Session};

% Middle row - Deployment Phase  
\node[data, below=of training] (deployment) {Deployment\\ Anchor};
\node[data, left=of deployment] (inference) {Inference\\ Receipt};

% Bottom left - Output Phase
\node[data, below=of inference] (lightweight) {Lightweight\\ Receipt};

% Bottom right - Storage Phase
\node[data, right=of lightweight] (capsule) {Capsule\\ Header};

% Process nodes positioned to avoid overlaps
\node[process, below left=1.2cm and 0.8cm of dataset] (materialize) {Materialize};
\node[process, below right=1.2cm and 0.8cm of capsule] (integrate) {Integrate};

% Primary flow arrows - the main lifecycle
\draw[arrow] (dataset) -- node[above, font=\tiny] {defines} (model);
\draw[arrow] (model) -- node[above, font=\tiny] {trains} (training);
\draw[arrow] (training) -- node[right, font=\tiny] {creates} (deployment);
\draw[arrow] (deployment) -- node[above, font=\tiny] {generates} (inference);
\draw[arrow] (inference) -- node[left, font=\tiny] {outputs} (lightweight);

% Secondary flow arrows - storage and materialization
\draw[arrow] (lightweight) -- node[below, font=\tiny] {stores in} (capsule);
\draw[arrow] (dataset) -- node[left, font=\tiny] {can} (materialize);
\draw[arrow] (materialize) -- node[below, font=\tiny] {retrieves} (lightweight);
\draw[arrow] (capsule) -- node[right, font=\tiny] {can} (integrate);
\draw[arrow] (integrate) -- node[above right, font=\tiny] {accesses} (inference);

% Phase groupings with better spacing
\begin{scope}[on background layer]
% Training Phase
\node[rectangle, draw=ciafgreen!40, thick, rounded corners=6pt, 
      fit={(dataset) (model) (training)}, 
      fill=ciafgreen!5, inner sep=8pt, 
      label={[font=\tiny, color=ciafgreen]above:Training Phase}] {};
      
% Deployment Phase  
\node[rectangle, draw=ciafdarkblue!40, thick, rounded corners=6pt, 
      fit={(deployment) (inference)}, 
      fill=ciafdarkblue!5, inner sep=8pt,
      label={[font=\tiny, color=ciafdarkblue]above:Deployment Phase}] {};
      
% Storage Phase
\node[rectangle, draw=ciafred!40, thick, rounded corners=6pt, 
      fit={(lightweight) (capsule) (materialize) (integrate)}, 
      fill=ciafred!5, inner sep=8pt,
      label={[font=\tiny, color=ciafred]below:Storage \& Access Phase}] {};
\end{scope}

\end{tikzpicture}
\caption{CIAF Data Structure Flow and Relationships}
\label{fig:dataflow}
\end{figure}

\subsection{Cryptographic Verification Chain}

The cryptographic verification chain demonstrates how data structures maintain integrity through the complete lifecycle:

\begin{align}
\text{Dataset Hash} &\rightarrow \text{Model Hash} \rightarrow \text{Training Hash} \\
&\rightarrow \text{Deployment Hash} \rightarrow \text{Inference Hash} \\
&\rightarrow \text{Capsule Hash} \rightarrow \text{Digital Signature}
\end{align}

Each hash in the chain is computed using SHA-256 with canonical JSON serialization, ensuring that any modification to the data structures is cryptographically detectable.

\newpage

\section{Storage and Serialization Patterns}

\subsection{Canonical Serialization}

All data structures implement canonical serialization to ensure consistent hash computation:

\begin{lstlisting}[language=Python, caption=Canonical JSON Serialization]
def canonical_json(obj: Any) -> str:
    """
    Create canonical JSON representation for cryptographic hashing.
    
    Args:
        obj: Object to serialize
        
    Returns:
        Canonical JSON string with sorted keys and no whitespace
    """
    return json.dumps(
        obj,
        sort_keys=True,           # Ensure deterministic key ordering
        separators=(',', ':'),    # Remove whitespace
        ensure_ascii=True,        # Use ASCII encoding
        default=str               # Convert non-serializable types to strings
    )

def canonical_hash(obj: Any) -> str:
    """
    Compute SHA-256 hash of canonical JSON representation.
    
    Args:
        obj: Object to hash
        
    Returns:
        Hexadecimal SHA-256 hash string
    """
    canonical_str = canonical_json(obj)
    return sha256_hash(canonical_str.encode('utf-8'))
\end{lstlisting}

\subsection{Storage Optimization Patterns}

The framework implements several storage optimization patterns:

\textbf{Differential Storage:} Only store changes between versions rather than complete data structures.

\textbf{Compression Integration:} Leverage compression algorithms for cold storage while maintaining hot storage performance.

\textbf{Reference Indirection:} Use cryptographic references instead of embedding large data structures directly.

\section{Validation and Error Handling}

\subsection{Schema Validation Framework}

Comprehensive validation ensures data structure integrity throughout the system:

\begin{lstlisting}[language=Python, caption=Receipt Validation Framework]
class ReceiptValidator:
    """Comprehensive validation for receipt data structures."""
    
    @staticmethod
    def validate_training_receipt(receipt: TrainingReceipt) -> List[str]:
        """
        Validate training receipt structure and constraints.
        
        Returns:
            List of validation error messages (empty if valid)
        """
        errors = []
        
        # Validate UUID format
        try:
            uuid.UUID(receipt.receipt_id)
        except ValueError:
            errors.append("Invalid receipt_id format")
        
        # Validate anchor formats (64-character hex strings)
        for anchor_name, anchor_value in [
            ("model_anchor_ref", receipt.model_anchor_ref)
        ]:
            if not re.match(r'^[a-f0-9]{64}$', anchor_value.lower()):
                errors.append(f"Invalid {anchor_name} format")
        
        # Validate Merkle path
        for i, path_element in enumerate(receipt.merkle_path):
            if not isinstance(path_element, list) or len(path_element) != 2:
                errors.append(f"Invalid Merkle path element at index {i}: must be [hash, position]")
            else:
                hash_val, position = path_element
                if not re.match(r'^[a-f0-9]{64}$', hash_val.lower()):
                    errors.append(f"Invalid Merkle path hash at index {i}")
                if position not in ["left", "right"]:
                    errors.append(f"Invalid Merkle path position at index {i}: must be 'left' or 'right'")
        
        # Validate timestamp format
        try:
            datetime.fromisoformat(receipt.committed_at.replace('Z', '+00:00'))
        except ValueError:
            errors.append("Invalid timestamp format")
        
        return errors
\end{lstlisting}

\subsection{Error Classification System}

The framework defines comprehensive error classification for debugging and monitoring:

\begin{lstlisting}[language=Python, caption=Error Classification System]
class CIAFDataStructureError(Exception):
    """Base exception for CIAF data structure errors."""
    pass

class ValidationError(CIAFDataStructureError):
    """Validation constraint violation."""
    pass

class SerializationError(CIAFDataStructureError):
    """JSON serialization/deserialization error."""
    pass

class CryptographicError(CIAFDataStructureError):
    """Cryptographic operation failure."""
    pass

class PolicyViolationError(CIAFDataStructureError):
    """Policy constraint violation."""
    pass
\end{lstlisting}

\section{Performance Analysis}

\subsection{Storage Efficiency Metrics}

Comprehensive analysis of storage efficiency across data structure types:

\begin{table}[H]
\centering
\caption{Data Structure Storage Analysis}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Data Structure} & \textbf{Typical Size} & \textbf{Compression Ratio} & \textbf{Materialization Cost} \\
\midrule
Lightweight Receipt & 0.5-1 KB & 100:1 & Low \\
LCM Inference Receipt & 2-5 KB & 10:1 & Medium \\
Dataset Anchor & 1-3 KB & 5:1 & High \\
Model Anchor & 3-8 KB & 3:1 & High \\
Training Session & 5-15 KB & 2:1 & Very High \\
Capsule Header & 10-50 KB & 1:1 & Very High \\
\bottomrule
\end{tabular}
\label{tab:storage}
\end{table}

\subsection{Computational Complexity Analysis}

Hash computation and verification complexity for each data structure type:

\begin{align}
\text{Hash Computation} &: O(n) \text{ where } n = \text{serialized size} \\
\text{Signature Generation} &: O(1) \text{ for Ed25519} \\
\text{Merkle Proof Verification} &: O(\log m) \text{ where } m = \text{tree size} \\
\text{Schema Validation} &: O(k) \text{ where } k = \text{field count}
\end{align}

\section{Implementation Guidelines}

\subsection{Best Practices}

\textbf{Immutability Enforcement:} All data structures should be treated as immutable once created. Use frozen dataclasses or immutable collections where possible.

\textbf{Lazy Loading:} Implement lazy loading for large data structures to minimize memory usage during normal operations.

\textbf{Batch Processing:} Process multiple data structures in batches to optimize cryptographic operations and storage I/O.

\textbf{Error Recovery:} Implement comprehensive error recovery mechanisms for corrupted or incomplete data structures.

\subsection{Security Considerations}

\textbf{Input Validation:} Validate all input data before creating data structures to prevent injection attacks.

\textbf{Sensitive Data Handling:} Use commitment schemes for sensitive data and implement secure deletion for temporary data.

\textbf{Timing Attack Prevention:} Use constant-time operations for cryptographic comparisons to prevent timing-based side-channel attacks.

\textbf{Memory Safety:} Clear sensitive data from memory immediately after use and avoid storing decrypted sensitive data in data structures.

\section{Conclusion}

\subsection{Summary of Contributions}

This technical specification provides the authoritative reference for all data structures used in the Cognitive Insight AI Framework and Lazy Capsule Materialization process. The comprehensive documentation covers:

\textbf{Complete Data Structure Inventory:} All 20+ core data structures with detailed field specifications, validation rules, and usage patterns.

\textbf{Cryptographic Integration:} Comprehensive specification of how cryptographic primitives integrate with data structures to provide verification and integrity guarantees.

\textbf{Schema Validation Framework:} Formal JSON schemas and validation frameworks to ensure data structure consistency across implementations.

\textbf{Performance Optimization:} Storage efficiency analysis and computational complexity specifications for optimal implementation strategies.

\subsection{Implementation Impact}

The standardized data structure specifications enable:

\textbf{Consistent Implementation:} Standardized data structures ensure consistent behavior across different programming languages and computing environments.

\textbf{Interoperability:} JSON schemas and canonical serialization enable seamless data exchange between different CIAF implementations.

\textbf{Regulatory Compliance:} Comprehensive audit trail data structures support compliance with diverse regulatory frameworks.

\textbf{Scalable Architecture:} Optimized data structures support enterprise-scale AI deployments with millions of daily operations.

\subsection{Future Evolution}

The data structure framework is designed for evolution while maintaining backward compatibility:

\textbf{Schema Versioning:} Built-in version fields enable gradual migration to enhanced data structure versions.

\textbf{Extension Points:} Flexible metadata fields and protocol interfaces support new requirements without breaking existing implementations.

\textbf{Post-Quantum Readiness:} Cryptographic abstraction layers enable migration to post-quantum algorithms as standards mature.

\textbf{Performance Optimization:} Modular design enables performance improvements without changing external interfaces.

The CIAF data structure framework provides a robust foundation for enterprise AI governance that balances cryptographic security, storage efficiency, and regulatory compliance while maintaining the flexibility necessary for diverse implementation environments and evolving requirements.

\newpage

\section*{References}

\begin{enumerate}
\item Greenwood, D.J. ``The Cognitive Insight AI Framework (CIAF): A Comprehensive Analysis of Lazy Capsule Materialization for Enterprise AI Governance.'' Cognitive Insight Research, 2025.

\item Greenwood, D.J. ``LCM Technical Disclosure: Lazy Capsule Materialization for AI Governance.'' Cognitive Insight Research, 2024.

\item National Institute of Standards and Technology. ``FIPS PUB 180-4: Secure Hash Standard (SHS).'' NIST, 2015.

\item Bernstein, D.J., et al. ``Ed25519: High-speed high-security signatures.'' \textit{Journal of Cryptographic Engineering}, vol. 2, no. 2, pp. 77-89, 2012.

\item Merkle, R.C. ``A Digital Signature Based on a Conventional Encryption Function.'' \textit{Advances in Cryptology — CRYPTO '87}, Springer-Verlag, 1988.

\item JSON Schema Specification. ``JSON Schema: A Media Type for Describing JSON Documents.'' Internet Engineering Task Force, Draft 2020-12.

\item International Organization for Standardization. ``ISO 8601:2019 Date and time — Representations for information interchange.'' ISO Standard, 2019.

\item Python Software Foundation. ``PEP 484 -- Type Hints.'' Python Enhancement Proposal, 2014.

\item Python Software Foundation. ``PEP 526 -- Variable Annotations.'' Python Enhancement Proposal, 2016.

\item Pydantic Development Team. ``Pydantic: Data validation and settings management using Python type annotations.'' \url{https://pydantic-docs.helpmanual.io/}, 2023.
\end{enumerate}

\newpage

\section{Canonical JSON Example}

\textbf{Schema-Aligned JSON Example} (fully canonical per Variables Reference):

\begin{lstlisting}[caption=Canonical JSON Example]
{
  "capsule_version": "1.0",
  "capsule_type": "audit_proof",
  "timestamp": "2025-10-22T00:00:00Z",
  "record": {
    "type": "inference",
    "metadata": {
      "request_id": "req_12345",
      "model_anchor_ref": "mod_abcd1234",
      "deployment_anchor_ref": "dep_efgh5678",
      "committed_at": "2025-10-22T00:00:00Z",
      "evidence_strength": "simulated"
    },
    "leaf_hash": "e3b0c44298fc1c149afbf4c8996fb924..."
  },
  "proofs": {
    "merkle_path": [
      ["9f86d081884c7d659a2feaa0c55ad015...", "right"],
      ["b6d81b360a5672d80c27430f39153e2c...", "left"]
    ],
    "merkle_root": "3a7bd3e2360a3d...",
    "inclusion_proof_valid": true
  },
  "anchor": { "capsule_id": "cap_001" },
  "verification": {
    "signature": "ed25519:abcd...xyz",
    "timestamp": "2025-10-22T00:00:00Z",
    "signer_id": "ciaf_production_key_001"
  }
}
\end{lstlisting}

Uses: \texttt{request\_id}, \texttt{*\_anchor\_ref}, \texttt{evidence\_strength}, pair-form \texttt{merkle\_path}, and RFC 3339 times.

\newpage

\section*{Author Information}

\textbf{Denzil James Greenwood} is the creator of the Cognitive Insight AI Framework and inventor of the Lazy Capsule Materialization process. This technical specification is based on comprehensive analysis of the production CIAF codebase and represents the canonical reference for data structure implementation.

\textbf{Institutional Affiliation:} Independent Researcher \\
\textbf{Contact:} founder@cognitiveinsight.ai \\
\textbf{ORCID:} [To be assigned]

\section*{Copyright Notice}

\begin{infobox}
\textbf{Legal Notice}\\
© 2025 Denzil James Greenwood \\
This technical specification, \textit{``CIAF Data Structures: Technical Specification for Lazy Capsule Materialization,''} \\
is licensed under the \href{https://creativecommons.org/licenses/by/4.0/}{Creative Commons Attribution 4.0 International License (CC BY 4.0)}.

All accompanying source code is released under the \href{https://www.apache.org/licenses/LICENSE-2.0}{Apache License 2.0}. \\
Trademarks: Cognitive Insight™ and LCM™ are unregistered trademarks used in connection with ongoing research on verifiable AI governance and auditability.
\end{infobox}

\end{document}